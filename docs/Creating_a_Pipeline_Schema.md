# Defining a Pipeline

## Fields

Fields function as an intermediate language between an Agent and the Pipeline. Fields are defined using a nested hierarchy. Anything at the "base"-level hierarchy should be in the "base" section. Any Field in a sub-section does not need a base. 

### Example Intermediate Language

A customer name is generated by the agent and depending on the agent input it will also populate various data filters. Also assume that the minimum amount should be 10,000 unless specified by the agent input.

```
{
    "customer_name": <customer name>,
    "data_filters": {
        "minimum_amount": <minimuma amount>,
        "earliest_date": <earliest date>,
        "latest_date": <latest date>
    }
}
```

### Fields Schema for Intermediate Language

```
"fields": {
    "base": {
        "customer_name": {
            "name": "customer_name",
            "type": "string",
            "required": true,
            "description": "Name of the Customer"
        }
    },
    "data_filters": {
        "minimum_amount": {
            "name": "minimum_amount",
            "type": "float",
            "required": false,
            "description": "Minimum amount to be included in search",
            "default": 10000.0
        },
        "earliest_date": {
            "name": "earliest_date",
            "type": "datetime",
            "required": false,
            "description": "Earliest date to be included in search"
        },
        "latest_date": {
            "name": "latest_date",
            "type": "datetime",
            "required": false,
            "description": "Latest date to be included in search"
        }
    }
}
```

### Generating with Utils

```
from dsl_spa.utils.schema import PipelineField, PipelineSchema

customer_name = Field(field_name = "customer_name", field_type = "string", required = True, description = "Name of the Customer")
minimum_amount = Field(field_name = "minimum_amount", field_type = "float", required = False, description = "Minimum Amount to be included in search", section_name = "data_filters", default = 10000.0)
earliest_date = Field(field_name = "earliest_date", field_type = "datetime", required = False, description = "Earliest date to be included in search", section_name = "data_filters")
latest_date = Field(field_name = "latest_date", field_type = "datetime", required = False, description = "Latest date to be included in search", section_name = "data_filters")

pipeline = PipelineSchema(pipeline_name = "Demo Pipeline", fields = [customer_name, minimum_amount,earliest_date,latest_date])
pipeline.get_schema()
```

Because of this "base" definition, "base" should not be used as a section in your intermediate language.

## Queries

The Query Structure is language agnostic. However, the specifics of the SQL language used should match with the type of connector it is using.

### Query Schema

```
"queries": [
    {
        "name": "Get Customer Data",
        "connector": "<connector name>",
        "sql_clauses": [
            {
                "sql": "SELECT amount, date FROM <TABLE NAME> ",
                "optional": false
            },
            {
                "sql": "WHERE customer_name = '{base.customer_name}' ",
                "optional": true,
                "field": "base.customer_name"
            },
            {
                "sql": "AND amount >= '{data_filters.minimum_amount}' ",
                "optional": true,
                "field": "data_filters.minimum_amount"
            },
            {
                "sql": "AND date >= '{data_filters.earliest_date}' ",
                "optional": true,
                "field": "data_filters.earliest_date"
            },
            {
                "sql": "AND date <= '{data_filters.latest_date}' ",
                "optional": true,
                "field": "data_filters.latest_date"
            }
        ]
    }
]
```

### Generating with Utils

```
from dsl_spa.utils.schema import SQLQuery
query = SQLQuery(query_name = "Get Customer Data", connector_name = "database_connector")
query.add_clause(sql_clause = "SELECT amount, date FROM <TABLE NAME> ", optional = False)
query.add_clause(sql_clause = "WHERE customer_name = '{base.customer_name}' ", optional = True, field_required = "base.customer_name")
query.add_clause(sql_clause = "AND amount >= '{data_filters.minimum_amount}' ", optional = True, field_required = "data_filters.minimum_amount")
query.add_clause(sql_clause = "AND date >= '{data_filters.earliest_date}' ", optional = True, field_required = "data_filters.earliest_date")
query.add_clause(sql_clause = "AND date <= '{data_filters.latest_date}' ", optional = True, field_required = "data_filters.latest_date")
query.generate_schema()
```

### Example

When the optional parameters is set to `true` for the sql_clause, the pipeline will check that the field if `field` exists before adding the `sql` to the query. 

Given this as the input to the pipeline

```
{
    "customer_name": "Superwise",
    "data_filters": {
        "earliest_date": "2023-01-01"
    }
}
```

The above query schema would generate

```
SELECT amount, date 
FROM <TABLE NAME> 
WHERE customer_name = 'Superwise' AND amount >= 10000.0 AND date >= '2023-01-01'
```

## CSVs

CSVs and Queries are both operated as Pandas Dataframes once loaded to the pipeline (saved in self.queries by default), but are loaded differently.

### CSV Schema

```
"csvs":[
    {
        "name": "<query name>",
        "connector": "<connector name>",
        "csv_name": "<csv filename>",
        "column_filters":
        [
            {
                "field": "base.customer_name",
                "column": "CustomerName",
                "value": "{base.customer_name}"
            }
        ]
    }
]
```

A CSV can be loaded with a simple column filter where any row in the csv with column name `column` will be excluded unless it has the value `value`. CSVs in the pipeline framework are more limited than other queries because they cannot create dynamic queries. However, you can implement the same functionality that a dynamic query would create with the dataset functionality of the pipeline.

### Generating with Utils

```
from dsl_spa.utils.schema import CSV
csv = CSV(csv_name = "Customer Data", connector_name = "csv_connector", "filename" = "customer_data.csv")
csv.add_column_filter(field_name = "base.customer_name", column_name = "Customer", value = "{base.customer_name}")
csv.generate_schema()
```

## Filters

Filters have their values loaded from a query and allow for Pipeline outputs to be filtered without needing to rerun data load operations (Queries, CSVs, or Custom Loaders). 

### Filter Schema

```
"filters":[
    {
        "name": "<filter 1 name>",
        "display_name": "<display name of filter 1>",
        "column_name": "<name of column in query to load filter 1 from>",
        "include_any": <true/false>,
        "query": <query_name>
    },
    {
        "name": "<filter 2 name>",
        "display_name": "<display name of filter 2>",
        "column_name": "<name of column in query to load filter 2 from>",
        "include_any": <true/false>,
        "query": <query_name>
    }
]
```

### Generating with Utils

```
from dsl_spa.utils.schema import Filter
filter = Filter(filter_name = "filter", display_name = "Filter", column_name = "column_name", query_name = "Get Customer Data", include_any = True)
filter.get_schema()
```

## Datasets

A dataset is a definition of data transformations to apply to a query result, to a different dataset, or to merge two datasets together. The allowed transformations include any function passed to the pipeline constructor (by default the pipeline loads all functions within dsl_spa.pipeline.pipeline_functions.pipeline_functions_dict) but can accept any dictionary that maps str -> func. A dataset can also apply data filters as defined in `filters` schema, and do column-wise arithmetic operations (add, subtract, multiply, divide).

### Dataset Creation Schemas

Every dataset should being with one of these four create schemas.

#### Create From Query (query or CSV) 

Loads the pandas dataframe for query `<query name>` 

```
{
    "type": "query",
    "name": "<query name>"
}
```

#### Create From Multiplexed Query (query or CSV)  

Loads the pandas dataframe for query `<query name>` based on the field `field name` found in the field input. If more than one field is found it will pick the first field in the `options` list.

```
{
    "type": "multiplex_query",
    "options": {
        "<field name 1>": "<query name>",
        .
        .
        .
        "<field name n>": "<query name>"
    }
}

```

#### Create From Dataset

```
{
    "type": "dataset",
    "name": "<dataset name>"
}
```

#### Merge Two Datasets

```
{
    "type": "merge",
    "dataset1": "<left dataset>",
    "dataset2": "<right dataset>",
    "how": "<left,right,inner,outer>",
    "left_on": "<column in dataset1 to merge on>",
    "right_on": "<column in dataset2 to merge on>"
}
```

### Operation Schemas

#### Apply Function

`function name` should be found in the `functions` dict passed to the pipeline constructor. `params` and `fields` are only required when the function the pipeline is calling has parameters. A `param` is a parameter to the function that does not change. A `field` is a paramter to the function that is mapped from the input fields to the pipeline.

```
{
    "type": "function",
    "name": "<function name>",
    "params": {
        "<param 1 name>": "<param 1 value>",
        .
        .
        .
        "<param n name>": "<param n value>",
    },
    "fields": {
        "<param 1 name>": "<field name>",
        .
        .
        .
        "<param n name>": "<field name>",
    }
}
```

#### Apply Filters

```
{
    "type": "filter",
    "filters": [
        "<filter 1 name>",
        .
        .
        .
        "<filter n name>"
    ]
}
```

#### Apply Arithmetic Operation

```
{
    "type": "arithmetic",
    "column": "<column name>",
    "operation": "<+,-,*,/>",
    "by": "<value>"
}
```

### Dataset Schema

The create and operation schemas can be combined in a dataset definition to create a dataset. Every dataset create definition should begin with a single dataset creation followed by 0 to many operation.

#### From Query

```
"datasets": [
    {
        "name": "<dataset name>",
        "create":[
            {
                "type": "query",
                "name": "<query name>"
            },
            {
                "type": "filter",
                "filters": [
                    "<filter 1 name>",
                    .
                    .
                    .
                    "<filter n name>"
                ]
            },
            {
                "type": "function",
                "name": "<function name>",
                "params": {
                    "<param 1 name>": "<param 1 value>",
                    .
                    .
                    .
                    "<param n name>": "<param n value>",
                },
                "fields": {
                    "<param 1 name>": "<field name>",
                    .
                    .
                    .
                    "<param n name>": "<field name>",
                }
            },
            {
                "type": "arithmetic",
                "column": "<column name>",
                "operation": "<+,-,*,/>",
                "by": "<value>"
            }
        ]
    }
]
```

#### Merging Two Datasets

```
"datasets": [
    {
        "name": "<dataset name>",
        "create":[
            {
                "type": "merge",
                "dataset1": "<left dataset>",
                "dataset2": "<right dataset>",
                "how": "<left,right,inner,outer>",
                "left_on": "<column in dataset1 to merge on>",
                "right_on": "<column in dataset2 to merge on>"
            },
            {
                "type": "function",
                "name": "<function name>",
                "params": {
                    "<param 1 name>": "<param 1 value>",
                    .
                    .
                    .
                    "<param n name>": "<param n value>",
                },
                "fields": {
                    "<param 1 name>": "<field name>",
                    .
                    .
                    .
                    "<param n name>": "<field name>",
                }
            },
            {
                "type": "arithmetic",
                "column": "<column name>",
                "operation": "<+,-,*,/>",
                "by": "<value>"
            }
        ]
    }
]

```

### Generating with Utils

#### From Query

```
from dsl_spa.utils.schema import Dataset
dataset = Dataset(dataset_name = "Customer Data")
dataset.create_from_query(query_name = "Get Customer Data")
dataset.add_filter(filters_list = ["filter 1 name","filter 2 name", "filter n name"])
function_fields = {
    "customer_name": "base.customer_name"
}
function_params = {
    "number_param": 10.0,
    "text_param": "example"
}
dataset.add_function(function_name = "<funtion name>", function_fields = function_fields, function_params = function_params)
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
dataset.generate_schema()
```

#### From Merge

```
from dsl_spa.utils.schema import Dataset
dataset = Dataset(dataset_name = "Merged Data")
dataset.merge_two_datasets(dataset1_name = "Left Dataset", dataset2_name = "Right Dataset", how = "inner", left_on "Customer Name", right_on = "CustomerID")
function_fields = {
    "customer_name": "base.customer_name"
}
function_params = {
    "number_param": 10.0,
    "text_param": "example"
}
dataset.add_function(function_name = "<funtion name>", function_fields = function_fields, function_params = function_params)
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
dataset.generate_schema()
```

# Pipelines

## Standard Pipeline

### Schema

```
{
    "pipeline_name": "Example Pipeline",
    "scope": "Example of a Pipeline",
    "scope_description": "Simple example of a pipeline for loading customer data through queries and csvs",
    "fields": {
        "base": {
            "customer_name": {
                "name": "customer_name",
                "type": "string",
                "required": true,
                "description": "Name of the Customer"
            }
        },
        "data_filters": {
            "minimum_amount": {
                "name": "minimum_amount",
                "type": "float",
                "required": false,
                "description": "Minimum amount to be included in search",
                "default": 10000.0
            },
            "earliest_date": {
                "name": "earliest_date",
                "type": "datetime",
                "required": false,
                "description": "Earliest date to be included in search"
            },
            "latest_date": {
                "name": "latest_date",
                "type": "datetime",
                "required": false,
                "description": "Latest date to be included in search"
            }
        }
    },
    "queries": [
        {
            "name": "Get Customer Data",
            "connector": "<connector name>",
            "sql_clauses": [
                {
                    "sql": "SELECT amount, date FROM <TABLE NAME> ",
                    "optional": false
                },
                {
                    "sql": "WHERE customer_name = '{base.customer_name}' ",
                    "optional": true,
                    "field": "base.customer_name"
                },
                {
                    "sql": "AND amount >= '{data_filters.minimum_amount}' ",
                    "optional": true,
                    "field": "data_filters.minimum_amount"
                },
                {
                    "sql": "AND date >= '{data_filters.earliest_date}' ",
                    "optional": true,
                    "field": "data_filters.earliest_date"
                },
                {
                    "sql": "AND date <= '{data_filters.latest_date}' ",
                    "optional": true,
                    "field": "data_filters.latest_date"
                }
            ]
        }
    ],
    "csvs":[
        {
            "name": "<query name>",
            "connector": "<connector name>",
            "csv_name": "<csv filename>",
            "column_filters":
            [
                {
                    "field": "base.customer_name",
                    "column": "CustomerName",
                    "value": "{base.customer_name}"
                }
            ]
        }
    ],
    "datasets": [
        {
            "name": "Customer Dataset",
            "create":[
                {
                    "type": "query",
                    "name": "Get Customer Data"
                },
                {
                    "type": "function",
                    "name": "<function name>",
                    "params": {
                        "<param 1 name>": "<param 1 value>",
                        .
                        .
                        .
                        "<param n name>": "<param n value>",
                    },
                    "fields": {
                        "<param 1 name>": "<field name>",
                        .
                        .
                        .
                        "<param n name>": "<field name>",
                    }
                },
                {
                    "type": "arithmetic",
                    "column": "<column name>",
                    "operation": "<+,-,*,/>",
                    "by": "<value>"
                }
            ]
        },
        {
            "name": "Left Dataset",
            "create":[
                {
                    "type": "query",
                    "name": "Get Customer Data"
                }
            ]
        },
        {
            "name": "Right Dataset",
            "create":[
                {
                    "type": "query",
                    "name": "Customers From CSV"
                }
            ]
        },
        {
            "name": "Merged Dataset",
            "create":[
                {
                    "type": "merge",
                    "dataset1": "Left Dataset",
                    "dataset2": "Right Dataset>",
                    "how": "inner",
                    "left_on": "customer_name",
                    "right_on": "customer_column"
                },
                {
                    "type": "arithmetic",
                    "column": "amount",
                    "operation": "*",
                    "by": 1.0
                }
            ]
        }
    ]
}
```

### Generating With Utils

```
from dsl_spa.utils.schema import PipelineField, SQLQuery, CSV, Filter, Dataset, StandardPipeline

# Define Fields
customer_name = Field(field_name = "customer_name", field_type = "string", required = True, description = "Name of the Customer")
minimum_amount = Field(field_name = "minimum_amount", field_type = "float", required = False, description = "Minimum Amount to be included in search", section_name = "data_filters", default = 10000.0)
earliest_date = Field(field_name = "earliest_date", field_type = "datetime", required = False, description = "Earliest date to be included in search", section_name = "data_filters")
latest_date = Field(field_name = "latest_date", field_type = "datetime", required = False, description = "Latest date to be included in search", section_name = "data_filters")
fields = [customer_name, minimum_amount,earliest_date,latest_date]

# Define Queries
query = SQLQuery(query_name = "Get Customer Data", connector_name = "database_connector")
query.add_clause(sql_clause = "SELECT amount, date FROM <TABLE NAME> ", optional = False)
query.add_clause(sql_clause = "WHERE customer_name = '{base.customer_name}' ", optional = True, field_required = "base.customer_name")
query.add_clause(sql_clause = "AND amount >= '{data_filters.minimum_amount}' ", optional = True, field_required = "data_filters.minimum_amount")
query.add_clause(sql_clause = "AND date >= '{data_filters.earliest_date}' ", optional = True, field_required = "data_filters.earliest_date")
query.add_clause(sql_clause = "AND date <= '{data_filters.latest_date}' ", optional = True, field_required = "data_filters.latest_date")
queries = [query]

# Define CSVs
csv = CSV(csv_name = "Customers From CSV", connector_name = "csv_connector", "filename" = "customer_data.csv")
csv.add_column_filter(field_name = "base.customer_name", column_name = "customer_column", value = "{base.customer_name}")
csvs = [csv]

# Define Datasets
query_dataset = Dataset(dataset_name = "Customer Data")
dataset.create_from_query(query_name = "Get Customer Data")
function_fields = {
    "customer_name": "base.customer_name"
}
function_params = {
    "number_param": 10.0,
    "text_param": "example"
}
dataset.add_function(function_name = "<funtion name>", function_fields = function_fields, function_params = function_params)
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
from dsl_spa.utils.schema import Dataset

left_dataset = Dataset(dataset_name = "Left Dataset")
left_dataset.create_from_query(query_name = "Get Customer Data")

right_dataset = Dataset(dataset_name = "Right Dataset")
right_dataset.create_from_query(query_name = "Customers From CSV")

merged_dataset = Dataset(dataset_name = "Merged Data")
dataset.merge_two_datasets(dataset1_name = "Left Dataset", dataset2_name = "Right Dataset", how = "inner", left_on "customer_name", right_on = "customer_column")
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
datasets = [query_dataset, left_dataset, right_dataset, merged_dataset]

# Define Standard Pipeline
scope = "Example of a Pipeline"
scope_description = "Simple example of a pipeline for loading customer data through queries and csvs"
pipeline = StandardPipeline(pipeline_name = "Example Pipeline", fields = fields, queries = queries, csvs = csvs, datasets = datasets, scope = scope, "scope_description" = scope_description)
schema = pipeline.get_schema()
```