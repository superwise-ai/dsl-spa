# Defining a Pipeline

## Fields

Fields function as an intermediate language between an Agent and the Pipeline. Fields are defined using a nested hierarchy. Anything at the "base"-level hierarchy should be in the "base" section. Any Field in a sub-section does not need a base. 
Each field should have:
 - name - Name of the field
 - type - Type of the field
 - required - Whether the field is required for the pipeline to run
 - description - A description of the value of the field

Optional values:
 - A default value - the value the field will be set to should it not be included in input field dictionary
 
 The type is used partially for documentation (although type validation is performed for str, float, and int types), but if declared as "categorical", DSL-SPA will reinterpret the field into a one-hot encoded boolean field. A quick example of a categorical field would be a field called `base.temperature_scale` where the value is either celsius or fahrenheight. If the value is celsius, a new field called `base.temperature_scale_celsius` will be created and set to `True` (likewise for `base.temperature_scale_fahrenheit` if it is set to fahrenheit). Note, a field for base.temperature_scale_fahrenheit will not be created if `base.temperature_scale` is set to celsius. Type checking is included for strings (must be "str" or "string), integers (must be "int" or "integer") and floating point numbers (must be "float" or "number). 

### Field Schema

```
{
    "name": <field name>,
    "type": <categorical/field type>,
    "required": <true/false>,
    "description": <a description of the value the field represents>,
    "default": <optional, the default value for the field>
}
```

### Example Intermediate Language (Response from LLM)

A customer name is generated by the agent and depending on the agent input it will also populate various data filters. Also assume that the minimum amount should be 10,000 unless specified by the agent input.

```
{
    "customer_name": <customer name>,
    "data_filters": {
        "minimum_amount": <minimuma amount>,
        "earliest_date": <earliest date>,
        "latest_date": <latest date>
    }
}
```

### Fields Schema for Intermediate Language

```
"fields": {
    "base": {
        "customer_name": {
            "name": "customer_name",
            "type": "string",
            "required": true,
            "description": "Name of the Customer"
        }
    },
    "data_filters": {
        "minimum_amount": {
            "name": "minimum_amount",
            "type": "float",
            "required": false,
            "description": "Minimum amount to be included in search",
            "default": 10000.0
        },
        "earliest_date": {
            "name": "earliest_date",
            "type": "datetime",
            "required": false,
            "description": "Earliest date to be included in search"
        },
        "latest_date": {
            "name": "latest_date",
            "type": "datetime",
            "required": false,
            "description": "Latest date to be included in search"
        }
    }
}
```

### Generating with Utils

```
from dsl_spa.utils.schema import PipelineField, PipelineSchema

customer_name = Field(field_name = "customer_name", field_type = "string", required = True, description = "Name of the Customer")
minimum_amount = Field(field_name = "minimum_amount", field_type = "float", required = False, description = "Minimum Amount to be included in search", section_name = "data_filters", default = 10000.0)
earliest_date = Field(field_name = "earliest_date", field_type = "datetime", required = False, description = "Earliest date to be included in search", section_name = "data_filters")
latest_date = Field(field_name = "latest_date", field_type = "datetime", required = False, description = "Latest date to be included in search", section_name = "data_filters")

pipeline = PipelineSchema(pipeline_name = "Demo Pipeline", fields = [customer_name, minimum_amount,earliest_date,latest_date])
pipeline.get_schema()
```

Because of this "base" definition, "base" should not be used as a section in your intermediate language.

## Queries

Queries are used by DSL-SPA to connect to data stores. Any data store that uses a querying language (I.E. SQL, NoSQL, etc...) can be worked with, the structure is database agnostic. However, the specifics of the SQL language used should match with the type of connector it is using.

Each query should have:

 - name - The name of the query to be used as a reference in other components
 - connector - The name of the connector as it is passed into the pipeline's connector dictionary
 - clauses - A list of clauses used to build the query string

Each clause in the `clauses` list should have:
 - clause - The query string for this specific clause
 - optional - Whether the query string is optional to include

If optional is set to `true` then there should be a `field` value set to the name of the field that determines if this clause is optional. Note that the clause is determined to be optional based on whether the field exists, not based on the value of the field. This is useful for defining data filters in the query (I.E. a WHERE clause in SQL).

### Query Schema

```
{
    "name": <name of the query>,
    "connector": <name of the connector>,
    "clauses": [
        {
            "clause": <query string for this clause>,
            "optional": false
        },
        {
            "clause": <query string for this clause>,
            "optional": true,
            "field": <field to determine if clause should be included>
        }
    ]
}
```

### Example Query Schema

```
"queries": [
    {
        "name": "Get Customer Data",
        "connector": "<connector name>",
        "clauses": [
            {
                "clause": "SELECT amount, date FROM <TABLE NAME> ",
                "optional": false
            },
            {
                "clause": "WHERE customer_name = '{base.customer_name}' ",
                "optional": true,
                "field": "base.customer_name"
            },
            {
                "clause": "AND amount >= '{data_filters.minimum_amount}' ",
                "optional": true,
                "field": "data_filters.minimum_amount"
            },
            {
                "clause": "AND date >= '{data_filters.earliest_date}' ",
                "optional": true,
                "field": "data_filters.earliest_date"
            },
            {
                "clause": "AND date <= '{data_filters.latest_date}' ",
                "optional": true,
                "field": "data_filters.latest_date"
            }
        ]
    }
]
```

### Generating with Utils

```
from dsl_spa.utils.schema import Query
query = Query(query_name = "Get Customer Data", connector_name = "database_connector")
query.add_clause(clause = "SELECT amount, date FROM <TABLE NAME> ", optional = False)
query.add_clause(clause = "WHERE customer_name = '{base.customer_name}' ", optional = True, field_required = "base.customer_name")
query.add_clause(clause = "AND amount >= '{data_filters.minimum_amount}' ", optional = True, field_required = "data_filters.minimum_amount")
query.add_clause(clause = "AND date >= '{data_filters.earliest_date}' ", optional = True, field_required = "data_filters.earliest_date")
query.add_clause(clause = "AND date <= '{data_filters.latest_date}' ", optional = True, field_required = "data_filters.latest_date")
query.generate_schema()
```

### Example

When the optional parameters is set to `true` for the clause, the pipeline will check that the field if `field` exists before adding the `clause` to the query. 

Given this as the input to the pipeline

```
{
    "customer_name": "Superwise",
    "data_filters": {
        "earliest_date": "2023-01-01"
    }
}
```

The above query schema would generate

```
SELECT amount, date 
FROM <TABLE NAME> 
WHERE customer_name = 'Superwise' AND amount >= 10000.0 AND date >= '2023-01-01'
```

## CSVs

CSVs and Queries are both operated as Pandas Dataframes once loaded to the pipeline (saved in self.queries by default), but are loaded differently. Since CSVs do not have queries for dynamic filtering, DSL-SPA has column filters that let a developer filter a CSV directly. 

A column filter should have three values:

 - field - the field to use for filtering (if this field is not included then this column filter will not be applied)
 - column - the column name from the CSV to apply the filtering to
 - value - the value to use for filtering 
 
If a developer wishes to filter the field value, the value can be set to the name of the field in {}. I.E. if the field is `base.customer_name` then value should be set to `{base.customer_name}`. This architecture allows for filtering based on categorical field values.

### CSV Schema

```
{
    "name": <query name>,
    "connector": <connector name>,
    "csv_name": <file name of csv>,
    "column_filters":
    [
        {
            "field": <field name to use as filter>,
            "column": <column name to filter on>,
            "value": <value to filter>
        }
    ]
}
```

### Example CSV Schema

```
"csvs":[
    {
        "name": "Customer Data",
        "connector": "csv_connector",
        "csv_name": "customer_data.csv",
        "column_filters":
        [
            {
                "field": "base.customer_name",
                "column": "CustomerName",
                "value": "{base.customer_name}"
            }
        ]
    }
]
```

### Generating with Utils

```
from dsl_spa.utils.schema import CSV
csv = CSV(csv_name = "Customer Data", connector_name = "csv_connector", "filename" = "customer_data.csv")
csv.add_column_filter(field_name = "base.customer_name", column_name = "Customer", value = "{base.customer_name}")
csv.generate_schema()
```

## Filters

Filters have their values loaded from a query and allow for Pipeline outputs to be filtered without needing to rerun data load operations (Queries, CSVs, or Custom Loaders). 

### Filter Schema

```
{
    "name": "<filter name>",
    "display_name": "<display name of filter>",
    "column_name": "<name of column in query to load filter from>",
    "include_any": <true/false>,
    "query": <query_name>
}
```

### Example Filter Schema

```
"filters":[
    {
        "name": "filter",
        "display_name": "Filter",
        "column_name": "column_name",
        "include_any": true,
        "query": "Get Customer Data"
    }
]
```

### Generating with Utils

```
from dsl_spa.utils.schema import Filter
filter = Filter(filter_name = "filter", display_name = "Filter", column_name = "column_name", query_name = "Get Customer Data", include_any = True)
filter.get_schema()
```

## Datasets

A dataset is a definition of data transformations to apply to a query result, to a different dataset, or to merge two datasets together. The allowed transformations include any function passed to the pipeline constructor (by default the pipeline loads all functions within dsl_spa.pipeline.pipeline_functions.pipeline_functions_dict) but can accept any dictionary that maps str -> func. A dataset can also apply data filters as defined in `filters` schema, and do column-wise arithmetic operations (add, subtract, multiply, divide).

### Dataset Creation Schemas

Every dataset should being with one of these four create schemas.

#### Create From Query (query or CSV) 

Loads the pandas dataframe for query `<query name>` 

```
{
    "type": "query",
    "name": "<query name>"
}
```

#### Create From Multiplexed Query (query or CSV)  

Loads the pandas dataframe for query `<query name>` based on the field `field name` found in the field input. If more than one field is found it will pick the first field in the `options` list.

```
{
    "type": "multiplex_query",
    "options": {
        "<field name 1>": "<query name>",
        .
        .
        .
        "<field name n>": "<query name>"
    }
}

```

#### Create From Dataset

```
{
    "type": "dataset",
    "name": "<dataset name>"
}
```

#### Merge Two Datasets

```
{
    "type": "merge",
    "dataset1": "<left dataset>",
    "dataset2": "<right dataset>",
    "how": "<left,right,inner,outer>",
    "left_on": "<column in dataset1 to merge on>",
    "right_on": "<column in dataset2 to merge on>"
}
```

### Operation Schemas

#### Apply Function

`function name` should be found in the `functions` dict passed to the pipeline constructor. `params` and `fields` are only required when the function the pipeline is calling has parameters. A `param` is a parameter to the function that does not change. A `field` is a paramter to the function that is mapped from the input fields to the pipeline.

```
{
    "type": "function",
    "name": "<function name>",
    "params": {
        "<param 1 name>": "<param 1 value>",
        .
        .
        .
        "<param n name>": "<param n value>",
    },
    "fields": {
        "<param 1 name>": "<field name>",
        .
        .
        .
        "<param n name>": "<field name>",
    }
}
```

#### Apply Filters

```
{
    "type": "filter",
    "filters": [
        "<filter 1 name>",
        .
        .
        .
        "<filter n name>"
    ]
}
```

#### Apply Arithmetic Operation

```
{
    "type": "arithmetic",
    "column": "<column name>",
    "operation": "<+,-,*,/>",
    "by": "<value>"
}
```

### Dataset Schema

The create and operation schemas can be combined in a dataset definition to create a dataset. Every dataset create definition should begin with a single dataset creation followed by 0 to many operation.

#### From Query

```
"datasets": [
    {
        "name": "<dataset name>",
        "create":[
            {
                "type": "query",
                "name": "<query name>"
            },
            {
                "type": "filter",
                "filters": [
                    "<filter 1 name>",
                    .
                    .
                    .
                    "<filter n name>"
                ]
            },
            {
                "type": "function",
                "name": "<function name>",
                "params": {
                    "<param 1 name>": "<param 1 value>",
                    .
                    .
                    .
                    "<param n name>": "<param n value>",
                },
                "fields": {
                    "<param 1 name>": "<field name>",
                    .
                    .
                    .
                    "<param n name>": "<field name>",
                }
            },
            {
                "type": "arithmetic",
                "column": "<column name>",
                "operation": "<+,-,*,/>",
                "by": "<value>"
            }
        ]
    }
]
```

#### Merging Two Datasets

```
"datasets": [
    {
        "name": "<dataset name>",
        "create":[
            {
                "type": "merge",
                "dataset1": "<left dataset>",
                "dataset2": "<right dataset>",
                "how": "<left,right,inner,outer>",
                "left_on": "<column in dataset1 to merge on>",
                "right_on": "<column in dataset2 to merge on>"
            },
            {
                "type": "function",
                "name": "<function name>",
                "params": {
                    "<param 1 name>": "<param 1 value>",
                    .
                    .
                    .
                    "<param n name>": "<param n value>",
                },
                "fields": {
                    "<param 1 name>": "<field name>",
                    .
                    .
                    .
                    "<param n name>": "<field name>",
                }
            },
            {
                "type": "arithmetic",
                "column": "<column name>",
                "operation": "<+,-,*,/>",
                "by": "<value>"
            }
        ]
    }
]

```

### Generating with Utils

#### From Query

```
from dsl_spa.utils.schema import Dataset
dataset = Dataset(dataset_name = "Customer Data")
dataset.create_from_query(query_name = "Get Customer Data")
dataset.add_filter(filters_list = ["filter 1 name","filter 2 name", "filter n name"])
function_fields = {
    "customer_name": "base.customer_name"
}
function_params = {
    "number_param": 10.0,
    "text_param": "example"
}
dataset.add_function(function_name = "<funtion name>", function_fields = function_fields, function_params = function_params)
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
dataset.generate_schema()
```

#### From Merge

```
from dsl_spa.utils.schema import Dataset
dataset = Dataset(dataset_name = "Merged Data")
dataset.merge_two_datasets(dataset1_name = "Left Dataset", dataset2_name = "Right Dataset", how = "inner", left_on "Customer Name", right_on = "CustomerID")
function_fields = {
    "customer_name": "base.customer_name"
}
function_params = {
    "number_param": 10.0,
    "text_param": "example"
}
dataset.add_function(function_name = "<funtion name>", function_fields = function_fields, function_params = function_params)
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
dataset.generate_schema()
```

## Summaries

Summaries are used to generate text outputs from datasets. The summary schema is split between the `summary` section and the `datasets` section. 

Each dataset that should be summarized should have:
 - summarize - the string used to create a summary for each row in the dataset
 - remove_comma - whether to remove the last comma (useful if generating a comma seperated list with each row to remove the last comma)

Column values from the dataset may be included in the `summarize` value by encasing it in {}. I.E. if there is a custom_name column and a total_sales column, the `summarize` value could look like `The total sales for customer {customer_name} is {total_sales}.` In order to add the name of the customer and their total sales to the summary.

Each dataset can also have:
 - prefix - A prefix to put before the row by row summary to add context to the summary
 - suffix - A suffix to put after the row by row summary to add context to the summary
 - empty_summary - The value to put in place if the dataset has no rows after querying and data tranformations.
 - required_fields - The two-dimensional OR-AND list defining the set of fields that must be included to generate the summary
 - exclude_fields - The two-dimensional OR-AND list defining the set of fields that must not be included to generate the summary

Prefix and Suffix can include field identifiers like `{base.customer_name}` that will be filled in with the value of the field. For `required_fields` and `exclude_fields`, the format to generate the logical expression (field 1 AND field 2 AND field 3) OR (field 4 AND field 5) OR (field 6) looks like:
```
[
    [<field 1>, <field 2>, <field 3>],
    [<field 4>, <field 5>],
    [<field 6>]
]
```

Each list is joined by an OR and each element in the list is joined by an AND.

On top of the schema included with each dataset, there is also dedicated schema for the summary. The summary should have:

 - datasets - The list of dataset names to build into the summary (order in the list dictates order in the summary)

Optionally the schema can also include:

 - prefix - Prefix to include at the beginning of the summary
 - suffix - Suffix to include at the end of the summary

### Dataset Schema for Summary

```
{
    "name": "<dataset name>",
    "create":[
        {
            "type": "query",
            "name": "<query name>"
        }
    ]
    "prefix": <The prefix of the summary>,
    "suffix": <The suffix of the summary>
    "empty_summary": <The summary to use if the dataset has no rows>,
    "required_fields": [
        [<field 1>, <field 2>, <field 3>],
        [<field 4>, <field 5>],
        [<field 6>]
    ],
    "exclude_fields": [
        [<field 2>, <field 3>, <field 4>],
        [<field 5>, <field 6>],
        [<field 1>]
    ]
}        
```

### Summary Schema

```
{
    "datasets": [<dataset 1>, <dataset 2>],
    "prefix": <prefix for the summary>,
    "suffix": <suffix for the summary>
}
```

## Visualizations

## Actions

Actions are used to define executable commands for the pipeline. Each action should map to a python function (more than one action can point to the same function using different args). Each action should have:

 - name - the name of the action as it is used in the pipeline
 - function - the name of the function as it is passed into the pipeline's function dictionary

An action can also optionally have:

 - attributes - a dictionary thats lets a developer include fields (and other custom values) as args for the Action's function
 - params - a dictionary that lets a developer set static values as args for the Action's function
 - connectors - a list mapping an arg name to a connector name allowing developers to pass connectors to the Action's function
 - field_strings - a dictionary allowing developers to pass strings with in-lined field values letting a developer pass custom strings as args for the Action's function (useful for things like logging)
 - output_field - The name of the field to save the output of the function to

 Each attribute has an associated name, field, and optional value. The attribute name should be the name of the arg in the function. If the attribute is set to optional, it is assumed that the function arg it is mapped to a default value. Output fields lets a developer store outputs from actions to be passed to future actions or to be used outside the pipeline.

 ### Action Schema

 ```
{
    "name": <action name>,
    "function": <name of the function in the pipeline's function dictionary>,
    "attributes": {
        <attribute name>: {
            "name": <attribute name which is also the name of the arg in the function>,
            "field": <name of field to include as arg>,
            "optional": <whether this attribute is optional for this Action>
        }
    },
    "params": {
        <param name>: <arg value>
    },
    "connectors": [
        {
            "name": <connector name>,
            "param": <name of the arg in the function>
        }
    ],
    "field_strings": {
        <arg name>: <string value to add fields to>
    },
    "output_field": <name of field to store output of Action in>
}
 ```

## Commands

A command is a list of one or more Actions to execute in order.

### Command Schema

```
{
    <command name>: [<action 1>, <action 2>]
}
```

# Pipelines

## Standard Pipeline

### Schema

```
{
    "pipeline_name": "Example Pipeline",
    "scope": "Example of a Pipeline",
    "scope_description": "Simple example of a pipeline for loading customer data through queries and csvs",
    "fields": {
        "base": {
            "customer_name": {
                "name": "customer_name",
                "type": "string",
                "required": true,
                "description": "Name of the Customer"
            }
        },
        "data_filters": {
            "minimum_amount": {
                "name": "minimum_amount",
                "type": "float",
                "required": false,
                "description": "Minimum amount to be included in search",
                "default": 10000.0
            },
            "earliest_date": {
                "name": "earliest_date",
                "type": "datetime",
                "required": false,
                "description": "Earliest date to be included in search"
            },
            "latest_date": {
                "name": "latest_date",
                "type": "datetime",
                "required": false,
                "description": "Latest date to be included in search"
            }
        }
    },
    "queries": [
        {
            "name": "Get Customer Data",
            "connector": "<connector name>",
            "sql_clauses": [
                {
                    "sql": "SELECT amount, date FROM <TABLE NAME> ",
                    "optional": false
                },
                {
                    "sql": "WHERE customer_name = '{base.customer_name}' ",
                    "optional": true,
                    "field": "base.customer_name"
                },
                {
                    "sql": "AND amount >= '{data_filters.minimum_amount}' ",
                    "optional": true,
                    "field": "data_filters.minimum_amount"
                },
                {
                    "sql": "AND date >= '{data_filters.earliest_date}' ",
                    "optional": true,
                    "field": "data_filters.earliest_date"
                },
                {
                    "sql": "AND date <= '{data_filters.latest_date}' ",
                    "optional": true,
                    "field": "data_filters.latest_date"
                }
            ]
        }
    ],
    "csvs":[
        {
            "name": "<query name>",
            "connector": "<connector name>",
            "csv_name": "<csv filename>",
            "column_filters":
            [
                {
                    "field": "base.customer_name",
                    "column": "CustomerName",
                    "value": "{base.customer_name}"
                }
            ]
        }
    ],
    "datasets": [
        {
            "name": "Customer Dataset",
            "create":[
                {
                    "type": "query",
                    "name": "Get Customer Data"
                },
                {
                    "type": "function",
                    "name": "<function name>",
                    "params": {
                        "<param 1 name>": "<param 1 value>",
                        .
                        .
                        .
                        "<param n name>": "<param n value>",
                    },
                    "fields": {
                        "<param 1 name>": "<field name>",
                        .
                        .
                        .
                        "<param n name>": "<field name>",
                    }
                },
                {
                    "type": "arithmetic",
                    "column": "<column name>",
                    "operation": "<+,-,*,/>",
                    "by": "<value>"
                }
            ]
        },
        {
            "name": "Left Dataset",
            "create":[
                {
                    "type": "query",
                    "name": "Get Customer Data"
                }
            ]
        },
        {
            "name": "Right Dataset",
            "create":[
                {
                    "type": "query",
                    "name": "Customers From CSV"
                }
            ]
        },
        {
            "name": "Merged Dataset",
            "create":[
                {
                    "type": "merge",
                    "dataset1": "Left Dataset",
                    "dataset2": "Right Dataset>",
                    "how": "inner",
                    "left_on": "customer_name",
                    "right_on": "customer_column"
                },
                {
                    "type": "arithmetic",
                    "column": "amount",
                    "operation": "*",
                    "by": 1.0
                }
            ]
        }
    ]
}
```

### Generating With Utils

```
from dsl_spa.utils.schema import PipelineField, SQLQuery, CSV, Filter, Dataset, StandardPipelineSchema

# Define Fields
customer_name = Field(field_name = "customer_name", field_type = "string", required = True, description = "Name of the Customer")
minimum_amount = Field(field_name = "minimum_amount", field_type = "float", required = False, description = "Minimum Amount to be included in search", section_name = "data_filters", default = 10000.0)
earliest_date = Field(field_name = "earliest_date", field_type = "datetime", required = False, description = "Earliest date to be included in search", section_name = "data_filters")
latest_date = Field(field_name = "latest_date", field_type = "datetime", required = False, description = "Latest date to be included in search", section_name = "data_filters")
fields = [customer_name, minimum_amount,earliest_date,latest_date]

# Define Queries
query = SQLQuery(query_name = "Get Customer Data", connector_name = "database_connector")
query.add_clause(sql_clause = "SELECT amount, date FROM <TABLE NAME> ", optional = False)
query.add_clause(sql_clause = "WHERE customer_name = '{base.customer_name}' ", optional = True, field_required = "base.customer_name")
query.add_clause(sql_clause = "AND amount >= '{data_filters.minimum_amount}' ", optional = True, field_required = "data_filters.minimum_amount")
query.add_clause(sql_clause = "AND date >= '{data_filters.earliest_date}' ", optional = True, field_required = "data_filters.earliest_date")
query.add_clause(sql_clause = "AND date <= '{data_filters.latest_date}' ", optional = True, field_required = "data_filters.latest_date")
queries = [query]

# Define CSVs
csv = CSV(csv_name = "Customers From CSV", connector_name = "csv_connector", "filename" = "customer_data.csv")
csv.add_column_filter(field_name = "base.customer_name", column_name = "customer_column", value = "{base.customer_name}")
csvs = [csv]

# Define Datasets
query_dataset = Dataset(dataset_name = "Customer Data")
dataset.create_from_query(query_name = "Get Customer Data")
function_fields = {
    "customer_name": "base.customer_name"
}
function_params = {
    "number_param": 10.0,
    "text_param": "example"
}
dataset.add_function(function_name = "<funtion name>", function_fields = function_fields, function_params = function_params)
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
from dsl_spa.utils.schema import Dataset

left_dataset = Dataset(dataset_name = "Left Dataset")
left_dataset.create_from_query(query_name = "Get Customer Data")

right_dataset = Dataset(dataset_name = "Right Dataset")
right_dataset.create_from_query(query_name = "Customers From CSV")

merged_dataset = Dataset(dataset_name = "Merged Data")
dataset.merge_two_datasets(dataset1_name = "Left Dataset", dataset2_name = "Right Dataset", how = "inner", left_on "customer_name", right_on = "customer_column")
dataset.add_arithetic_operation(arithmetic_operation = "*", column = "amount", by = 1.0)
datasets = [query_dataset, left_dataset, right_dataset, merged_dataset]

# Define Standard Pipeline
scope = "Example of a Pipeline"
scope_description = "Simple example of a pipeline for loading customer data through queries and csvs"
pipeline = StandardPipelineSchema(pipeline_name = "Example Pipeline", fields = fields, queries = queries, csvs = csvs, datasets = datasets, scope = scope, "scope_description" = scope_description)
schema = pipeline.get_schema()
```